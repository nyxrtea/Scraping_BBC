# -*- coding: utf-8 -*-
"""baru.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gtw-f51et61CbgQEQ6QN5XOu9Bb1-lcu
"""

# Modul yang digunakan untuk wraping

import requests as rs
from bs4 import BeautifulSoup
import pandas as pd
import csv
import os

import requests as rs
from bs4 import BeautifulSoup

judul = []
url = 'https://www.bbc.com/news'

response = rs.get(url)

# Memeriksa permintaan
if response.status_code == 200:
    soup = BeautifulSoup(response.text, 'html.parser')

    # Mencari element di h3 dan h2
    conteny = soup.find_all(['h3', 'h2'])  # Memberi batasan class untuk mencari element

    for t in conteny:
        judul_text = t.get_text(strip=True)
        judul.append(judul_text)
else:
    print(f"Failed to retrieve page: Status code {response.status_code}")

print(judul)

# List untuk menyimpan deskripsi
deskripsi_list = []

# Gabungkan hasil select dari h3 dan h2
headlines = soup.select('h3') + soup.select('h2')

# Ambil deskripsi (jika tersedia) yang terkait dengan headline
for headline in headlines:
    # Cari elemen parent <a> untuk konteks headline
    link_tag = headline.find_parent('a')

    if link_tag:
        # Ambil deskripsi dari elemen <p> yang berada setelah <a>
        deskripsi_tag = link_tag.find_next('p')
        deskripsi_text = deskripsi_tag.get_text(strip=True) if deskripsi_tag else 'No description found'

        # Cek jika deskripsi belum ada dalam list
        if not any(desc['Deskripsi'] == deskripsi_text for desc in deskripsi_list):
            deskripsi_list.append({
                "Deskripsi": deskripsi_text
            })

# Output hasil
if not deskripsi_list:
    print("Tidak ada deskripsi yang ditemukan.")
else:
    for desc in deskripsi_list:
        print(f"Deskripsi: {desc['Deskripsi']}\n")

headlines = soup.select('h3, h2')
updates = [] #list untuk menyimpan deskripsi

for headline in headlines:
    link_tag = headline.find_parent('a')

    if link_tag:
        span_tags = link_tag.find_parent().find_all('span')

        last_update_text = 'Last update not found'
        for span in span_tags:
            text = span.get_text(strip=True)
            if text and len(text.split()) > 1:
                last_update_text = text
                break


        updates.append(last_update_text)


if not updates:
    print("Last update not found.")
else:
    for update in updates:
        print(update)

# Ambil elemen h3 atau h2 dari halaman
headlines = soup.select('h3') + soup.select('h2')

# List untuk menyimpan deskripsi link
link_list = []

for headline in headlines:
    # Cari elemen parent <a> untuk konteks headline
    link_tag = headline.find_parent('a')

    if link_tag:
        link = link_tag['href']
        # Jika tautannya relatif, tambahkan domain utama BBC
        full_link = f"https://www.bbc.com{link}" if link.startswith('/') else link
        link_list.append(full_link)  # Tambahkan ke list

# Output hasil
if not link_list:
    print("Tidak ada link yang ditemukan.")
else:
    for link in link_list:
        print(link)

# Cari semua elemen gambar
image_tags = soup.find_all('img')

# List untuk menyimpan URL gambar
image = []

# Ambil URL gambar
for img in image_tags:
    image_src = img.get('src')
    if image_src:
        # Jika src relatif, tambahkan domain BBC
        if image_src.startswith('//'):
            full_image_url = f"https:{image_src}"  # Tambahkan "https:" di depan
        elif image_src.startswith('/'):
            full_image_url = f"https://www.bbc.com{image_src}"  # Tambahkan domain utama
        else:
            full_image_url = image_src  # Sudah berupa URL

        # Filter out PNG images
        if not full_image_url.lower().endswith('.png'):
            image.append(full_image_url)

# Cek apakah URL ditemukan
if not image:
    print("Tidak ada URL gambar yang ditemukan.")
else:
    # Tampilkan URL gambar
    for url in image:
        print(url)

data= pd.DataFrame(zip(judul,deskripsi_list,updates,link,image),columns=['judul','deskripsi','last update','link','Gambar',])
data
data.to_csv('BBC data.csv',index=True)